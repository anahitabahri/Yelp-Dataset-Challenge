{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import time\n",
    "import glob\n",
    "import datetime\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../yelp-data/new_data/final_data/manip_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>open</th>\n",
       "      <th>review_count</th>\n",
       "      <th>BusinessStars</th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>user_fans</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_review_count</th>\n",
       "      <th>user_votes_cool</th>\n",
       "      <th>user_votes_funny</th>\n",
       "      <th>user_votes_useful</th>\n",
       "      <th>user_yelping_since</th>\n",
       "      <th>rating1</th>\n",
       "      <th>rating3</th>\n",
       "      <th>rating5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>Dravosburg</td>\n",
       "      <td>40.354327</td>\n",
       "      <td>-79.900706</td>\n",
       "      <td>Mr Hoagie</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016-04-08</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Jim</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>2012-10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>Dravosburg</td>\n",
       "      <td>40.354327</td>\n",
       "      <td>-79.900706</td>\n",
       "      <td>Mr Hoagie</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>PA</td>\n",
       "      <td>2016-04-10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UsFtqoBl7naz8AVUBZMjQQ</td>\n",
       "      <td>Dravosburg</td>\n",
       "      <td>40.350553</td>\n",
       "      <td>-79.886814</td>\n",
       "      <td>Clancy's Pub</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>2014-11-28</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Molly</td>\n",
       "      <td>158</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>161</td>\n",
       "      <td>2012-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buOw4-D2dVxOS_DOC4q8OQ</td>\n",
       "      <td>Homestead</td>\n",
       "      <td>40.409064</td>\n",
       "      <td>-79.915797</td>\n",
       "      <td>Yokoso Japanese Steakhouse</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>2014-10-02</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Molly</td>\n",
       "      <td>158</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>161</td>\n",
       "      <td>2012-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SsGNAc9U-aKPZccnaDtFkA</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>40.443145</td>\n",
       "      <td>-80.001104</td>\n",
       "      <td>Meat &amp; Potatoes</td>\n",
       "      <td>True</td>\n",
       "      <td>1175</td>\n",
       "      <td>4.0</td>\n",
       "      <td>PA</td>\n",
       "      <td>2014-09-13</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Molly</td>\n",
       "      <td>158</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>161</td>\n",
       "      <td>2012-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        city   latitude  longitude  \\\n",
       "0  5UmKMjUEUNdYWqANhGckJw  Dravosburg  40.354327 -79.900706   \n",
       "1  5UmKMjUEUNdYWqANhGckJw  Dravosburg  40.354327 -79.900706   \n",
       "2  UsFtqoBl7naz8AVUBZMjQQ  Dravosburg  40.350553 -79.886814   \n",
       "3  buOw4-D2dVxOS_DOC4q8OQ   Homestead  40.409064 -79.915797   \n",
       "4  SsGNAc9U-aKPZccnaDtFkA  Pittsburgh  40.443145 -80.001104   \n",
       "\n",
       "                         name  open  review_count  BusinessStars state  \\\n",
       "0                   Mr Hoagie  True             7            3.5    PA   \n",
       "1                   Mr Hoagie  True             7            3.5    PA   \n",
       "2                Clancy's Pub  True             5            3.0    PA   \n",
       "3  Yokoso Japanese Steakhouse  True            99            3.0    PA   \n",
       "4             Meat & Potatoes  True          1175            4.0    PA   \n",
       "\n",
       "         date   ...    user_fans  user_name user_review_count user_votes_cool  \\\n",
       "0  2016-04-08   ...            1        Jim                26               3   \n",
       "1  2016-04-10   ...            0   Jennifer                 3               0   \n",
       "2  2014-11-28   ...           10      Molly               158              80   \n",
       "3  2014-10-02   ...           10      Molly               158              80   \n",
       "4  2014-09-13   ...           10      Molly               158              80   \n",
       "\n",
       "   user_votes_funny  user_votes_useful  user_yelping_since  rating1  rating3  \\\n",
       "0                10                 26             2012-10        0        0   \n",
       "1                 0                  1             2016-04        1        0   \n",
       "2                46                161             2012-06        0        0   \n",
       "3                46                161             2012-06        0        1   \n",
       "4                46                161             2012-06        0        0   \n",
       "\n",
       "   rating5  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156695, 35)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important words for rating 1 vs rating 5, rating 3 vs rating 5..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rating1 = data[(data.rating1 == 1) | (data.rating5 == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rating3 = data[(data.rating3 == 1) | (data.rating5 == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74090, 35)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rating1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82716, 35)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rating3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a random forest model to predict review rating of 1 using the review text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert rows with empty review text to texts that are of the string variable type\n",
    "texts = data_rating1['text'].fillna('')\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[1, 2], \n",
    "                             stop_words='english',\n",
    "                             binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.97379081  0.97691817  0.98190105], Average AUC 0.977536677793\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 50)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating1['rating1']\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Features  Importance Score\n",
      "985          worst          0.040963\n",
      "409       horrible          0.033543\n",
      "882       terrible          0.028744\n",
      "369          great          0.026528\n",
      "26         amazing          0.020993\n",
      "213      delicious          0.020407\n",
      "38           asked          0.017961\n",
      "517        manager          0.015752\n",
      "730           rude          0.014998\n",
      "76            best          0.014978\n",
      "52           awful          0.014082\n",
      "55             bad          0.013582\n",
      "544        minutes          0.013532\n",
      "898           told          0.012020\n",
      "162           cold          0.011811\n",
      "88           bland          0.011427\n",
      "658           poor          0.011145\n",
      "628        perfect          0.010438\n",
      "733           said          0.009448\n",
      "269      excellent          0.008570\n",
      "553          money          0.007982\n",
      "528       mediocre          0.007692\n",
      "232  disappointing          0.007517\n",
      "961          waste          0.007482\n",
      "630      perfectly          0.006818\n",
      "601        ordered          0.006569\n",
      "586             ok          0.006317\n",
      "51         awesome          0.006300\n",
      "211     definitely          0.006297\n",
      "501           love          0.006207\n",
      "219           didn          0.005453\n",
      "247            dry          0.005125\n",
      "948         waited          0.004903\n",
      "466           left          0.004847\n",
      "319           food          0.004790\n",
      "295       favorite          0.004654\n",
      "337       friendly          0.004649\n",
      "952       waitress          0.004544\n",
      "902           took          0.004070\n",
      "907          tough          0.004003\n",
      "609     overcooked          0.003900\n",
      "810          sorry          0.003896\n",
      "875    tasted like          0.003870\n",
      "937          vegas          0.003870\n",
      "361           good          0.003603\n",
      "874         tasted          0.003520\n",
      "503          loved          0.003497\n",
      "835          steak          0.003489\n",
      "119           came          0.003459\n",
      "804           slow          0.003401\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.88045894  0.89104816  0.89771035], Average AUC 0.889739150836\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[2, 2], \n",
    "                             stop_words='english',\n",
    "                             binary=False)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating1['rating1']\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Features  Importance Score\n",
      "870        tasted like          0.020736\n",
      "2           20 minutes          0.012416\n",
      "566      minutes later          0.012254\n",
      "516        looked like          0.010875\n",
      "958        waste money          0.010335\n",
      "875   terrible service          0.009262\n",
      "904       took forever          0.009217\n",
      "446   horrible service          0.009088\n",
      "982      worst service          0.008735\n",
      "1           15 minutes          0.008702\n",
      "324     food poisoning          0.008571\n",
      "651       poor service          0.008518\n",
      "0           10 minutes          0.008324\n",
      "762   service horrible          0.008124\n",
      "959         waste time          0.008047\n",
      "773   service terrible          0.007802\n",
      "3           30 minutes          0.007329\n",
      "441   highly recommend          0.006870\n",
      "729         save money          0.006644\n",
      "39            bad food          0.006503\n",
      "805          stay away          0.006310\n",
      "41         bad service          0.005900\n",
      "5           45 minutes          0.005845\n",
      "950        walked away          0.005612\n",
      "770       service slow          0.005437\n",
      "888         time money          0.005350\n",
      "601     ordered medium          0.004757\n",
      "602      ordered steak          0.004732\n",
      "141  cooked perfection          0.004579\n",
      "854            sub par          0.004517\n",
      "414      great service          0.004340\n",
      "142   cooked perfectly          0.004322\n",
      "403         great food          0.004206\n",
      "761      service great          0.003770\n",
      "682         really bad          0.003747\n",
      "573       needless say          0.003709\n",
      "165   customer service          0.003649\n",
      "524         love place          0.003575\n",
      "210          didn want          0.003516\n",
      "488          las vegas          0.003408\n",
      "666          prime rib          0.003345\n",
      "4           40 minutes          0.003317\n",
      "113       cheese steak          0.003305\n",
      "686        really good          0.003286\n",
      "239          don waste          0.003269\n",
      "289       finally came          0.003236\n",
      "388           got food          0.003206\n",
      "316         food great          0.003185\n",
      "314          food good          0.003178\n",
      "328       food service          0.003111\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.87982498  0.89053495  0.89701698], Average AUC 0.88912563991\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[2, 3], \n",
    "                             stop_words='english',\n",
    "                             binary=False)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating1['rating1']\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Features  Importance Score\n",
      "871        tasted like          0.019834\n",
      "568      minutes later          0.013159\n",
      "2           20 minutes          0.012823\n",
      "958        waste money          0.010803\n",
      "518        looked like          0.010628\n",
      "903       took forever          0.009898\n",
      "981      worst service          0.009527\n",
      "442   horrible service          0.009520\n",
      "0           10 minutes          0.008985\n",
      "1           15 minutes          0.008979\n",
      "876   terrible service          0.008636\n",
      "759   service horrible          0.008579\n",
      "657       poor service          0.008286\n",
      "320     food poisoning          0.008270\n",
      "770   service terrible          0.007870\n",
      "3           30 minutes          0.007367\n",
      "959         waste time          0.007177\n",
      "437   highly recommend          0.006933\n",
      "802          stay away          0.006770\n",
      "36            bad food          0.006401\n",
      "38         bad service          0.006337\n",
      "724         save money          0.006262\n",
      "951        walked away          0.006198\n",
      "887         time money          0.006137\n",
      "767       service slow          0.005923\n",
      "5           45 minutes          0.005765\n",
      "608      ordered steak          0.005230\n",
      "854            sub par          0.004726\n",
      "136  cooked perfection          0.004553\n",
      "137   cooked perfectly          0.004338\n",
      "409      great service          0.004333\n",
      "606     ordered medium          0.004268\n",
      "576       needless say          0.004217\n",
      "399         great food          0.003930\n",
      "757      service great          0.003832\n",
      "527         love place          0.003750\n",
      "281       finally came          0.003658\n",
      "669          prime rib          0.003620\n",
      "684         really bad          0.003600\n",
      "161   customer service          0.003516\n",
      "487          las vegas          0.003464\n",
      "687        really good          0.003441\n",
      "234          don waste          0.003405\n",
      "4           40 minutes          0.003308\n",
      "207          didn want          0.003239\n",
      "324       food service          0.003237\n",
      "382           got food          0.003221\n",
      "309          food good          0.003129\n",
      "622   perfectly cooked          0.003107\n",
      "312         food great          0.003100\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a random forest model to predict review rating of 3 using the review text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert rows with empty review text to texts that are of the string variable type\n",
    "texts = data_rating3['text'].fillna('')\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[1, 2], \n",
    "                             stop_words='english',\n",
    "                             binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.89859042  0.91018894  0.90847058], Average AUC 0.905749980375\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 50)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating3['rating3']\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Features  Importance Score\n",
      "588             ok          0.031197\n",
      "369           good          0.022528\n",
      "218         decent          0.021808\n",
      "25         amazing          0.014353\n",
      "963           wasn          0.013568\n",
      "668         pretty          0.012586\n",
      "49         average          0.012062\n",
      "77            best          0.011559\n",
      "589           okay          0.011172\n",
      "55             bad          0.010806\n",
      "610        overall          0.009565\n",
      "222      delicious          0.009485\n",
      "80          better          0.008588\n",
      "378          great          0.008451\n",
      "629        perfect          0.007376\n",
      "831          stars          0.007247\n",
      "229           didn          0.007053\n",
      "533       mediocre          0.006755\n",
      "669    pretty good          0.006327\n",
      "89           bland          0.006316\n",
      "804           slow          0.005833\n",
      "242  disappointing          0.005818\n",
      "527          maybe          0.005773\n",
      "452        just ok          0.005485\n",
      "451           just          0.005052\n",
      "332      food good          0.004864\n",
      "328           food          0.004847\n",
      "836          steak          0.004843\n",
      "481           like          0.004814\n",
      "507           love          0.004781\n",
      "52         awesome          0.004453\n",
      "258            dry          0.004431\n",
      "603        ordered          0.004337\n",
      "304       favorite          0.004140\n",
      "85             bit          0.004002\n",
      "280      excellent          0.003945\n",
      "670          price          0.003943\n",
      "644          place          0.003887\n",
      "926  unfortunately          0.003818\n",
      "775        service          0.003725\n",
      "693         really          0.003619\n",
      "891          think          0.003618\n",
      "487         little          0.003612\n",
      "631      perfectly          0.003310\n",
      "940          vegas          0.003301\n",
      "982      wonderful          0.003186\n",
      "388          guess          0.003181\n",
      "221     definitely          0.003103\n",
      "909          tough          0.003080\n",
      "549        minutes          0.003012\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.79163955  0.81182261  0.80785771], Average AUC 0.803773287276\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[2, 2], \n",
    "                             stop_words='english',\n",
    "                             binary=False)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating3['rating3']\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Features  Importance Score\n",
      "665        pretty good          0.024973\n",
      "469            just ok          0.017386\n",
      "298          food good          0.014241\n",
      "304            food ok          0.009893\n",
      "438   highly recommend          0.007771\n",
      "773       service good          0.006958\n",
      "470          just okay          0.006957\n",
      "290        food decent          0.006393\n",
      "882        tasted like          0.005988\n",
      "929          ve better          0.005537\n",
      "958           wasn bad          0.005261\n",
      "399         great food          0.005132\n",
      "429         happy hour          0.004883\n",
      "989         write home          0.004868\n",
      "785       service slow          0.004708\n",
      "461          just didn          0.004606\n",
      "779         service ok          0.004563\n",
      "526         love place          0.004481\n",
      "359         good great          0.004474\n",
      "605       overall good          0.004464\n",
      "440           hit miss          0.004307\n",
      "673          prime rib          0.004149\n",
      "476          just wasn          0.004103\n",
      "135  cooked perfection          0.004071\n",
      "694        really good          0.003919\n",
      "224          don think          0.003890\n",
      "774      service great          0.003859\n",
      "58          best steak          0.003376\n",
      "409      great service          0.003331\n",
      "548    mashed potatoes          0.003237\n",
      "961     wasn impressed          0.003193\n",
      "533         mac cheese          0.003166\n",
      "677       probably won          0.003123\n",
      "300         food great          0.003121\n",
      "830         steak eggs          0.003102\n",
      "597     ordered medium          0.003099\n",
      "486          las vegas          0.003085\n",
      "308        food pretty          0.003055\n",
      "559        medium rare          0.003004\n",
      "284       food amazing          0.002943\n",
      "355          good food          0.002895\n",
      "664      pretty decent          0.002830\n",
      "315          food wasn          0.002812\n",
      "834         steak good          0.002725\n",
      "836        steak house          0.002706\n",
      "730          salad bar          0.002680\n",
      "720            rib eye          0.002672\n",
      "107       cheese steak          0.002671\n",
      "2           20 minutes          0.002656\n",
      "849     steak sandwich          0.002598\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a random forest model to predict review rating of 5 using the review text features (vs. rating of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rating5 = data[(data.rating5 == 1) | (data.rating1 == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = data_rating5['text'].fillna('')\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[1, 2], \n",
    "                             stop_words='english',\n",
    "                             binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.97300971  0.97595978  0.9822415 ], Average AUC 0.977070331065\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 50)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating5['rating5']\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Features  Importance Score\n",
      "985          worst          0.047057\n",
      "882       terrible          0.027668\n",
      "409       horrible          0.026447\n",
      "369          great          0.025806\n",
      "213      delicious          0.021186\n",
      "38           asked          0.019083\n",
      "26         amazing          0.018706\n",
      "55             bad          0.017699\n",
      "517        manager          0.017416\n",
      "544        minutes          0.017036\n",
      "76            best          0.016461\n",
      "52           awful          0.015954\n",
      "730           rude          0.014999\n",
      "898           told          0.013819\n",
      "162           cold          0.010581\n",
      "88           bland          0.010364\n",
      "628        perfect          0.010074\n",
      "232  disappointing          0.009908\n",
      "269      excellent          0.009576\n",
      "658           poor          0.009389\n",
      "733           said          0.007607\n",
      "961          waste          0.006752\n",
      "601        ordered          0.006557\n",
      "51         awesome          0.006539\n",
      "948         waited          0.006421\n",
      "528       mediocre          0.006373\n",
      "553          money          0.006305\n",
      "501           love          0.006188\n",
      "211     definitely          0.006082\n",
      "630      perfectly          0.005869\n",
      "219           didn          0.005806\n",
      "586             ok          0.005378\n",
      "466           left          0.004896\n",
      "952       waitress          0.004853\n",
      "337       friendly          0.004648\n",
      "609     overcooked          0.004614\n",
      "247            dry          0.004563\n",
      "319           food          0.004379\n",
      "875    tasted like          0.004337\n",
      "306        finally          0.004303\n",
      "295       favorite          0.004214\n",
      "937          vegas          0.004180\n",
      "804           slow          0.004022\n",
      "907          tough          0.003994\n",
      "361           good          0.003767\n",
      "119           came          0.003596\n",
      "835          steak          0.003500\n",
      "291      fantastic          0.003350\n",
      "503          loved          0.003307\n",
      "614           paid          0.003112\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC [ 0.88046813  0.89077663  0.89883119], Average AUC 0.890025318711\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=[2, 2], \n",
    "                             stop_words='english',\n",
    "                             binary=False)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the review text\n",
    "vectorizer.fit(texts)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - \n",
    "# one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(texts).todense()\n",
    "y = data_rating5['rating5']\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Features  Importance Score\n",
      "870        tasted like          0.019556\n",
      "2           20 minutes          0.012483\n",
      "566      minutes later          0.012194\n",
      "446   horrible service          0.011172\n",
      "516        looked like          0.010799\n",
      "958        waste money          0.010659\n",
      "904       took forever          0.010091\n",
      "0           10 minutes          0.010079\n",
      "982      worst service          0.009291\n",
      "875   terrible service          0.009055\n",
      "762   service horrible          0.008983\n",
      "773   service terrible          0.008566\n",
      "651       poor service          0.008160\n",
      "324     food poisoning          0.007733\n",
      "1           15 minutes          0.007450\n",
      "729         save money          0.007266\n",
      "441   highly recommend          0.006968\n",
      "3           30 minutes          0.006893\n",
      "959         waste time          0.006687\n",
      "39            bad food          0.006257\n",
      "805          stay away          0.006076\n",
      "950        walked away          0.006021\n",
      "770       service slow          0.005762\n",
      "41         bad service          0.005750\n",
      "5           45 minutes          0.005345\n",
      "601     ordered medium          0.005076\n",
      "888         time money          0.004995\n",
      "602      ordered steak          0.004994\n",
      "854            sub par          0.004692\n",
      "141  cooked perfection          0.004461\n",
      "414      great service          0.004353\n",
      "403         great food          0.004282\n",
      "142   cooked perfectly          0.004278\n",
      "388           got food          0.004014\n",
      "573       needless say          0.003889\n",
      "4           40 minutes          0.003767\n",
      "761      service great          0.003682\n",
      "524         love place          0.003679\n",
      "239          don waste          0.003674\n",
      "682         really bad          0.003640\n",
      "666          prime rib          0.003485\n",
      "165   customer service          0.003455\n",
      "113       cheese steak          0.003327\n",
      "488          las vegas          0.003244\n",
      "162        credit card          0.003242\n",
      "314          food good          0.003231\n",
      "615   perfectly cooked          0.003227\n",
      "28        asked wanted          0.003218\n",
      "210          didn want          0.003195\n",
      "66          best steak          0.003151\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)\n",
    "feature_importances = pd.DataFrame({'Features' : vectorizer.get_feature_names(), \n",
    "                                    'Importance Score': model.feature_importances_})\n",
    "print feature_importances.sort_values('Importance Score', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
