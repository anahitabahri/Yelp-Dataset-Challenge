{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../yelp-data/new_data/final_data/manip_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=False,\n",
    "                     stop_words='english',\n",
    "                     min_df=3)\n",
    "\n",
    "docs = cv.fit_transform(data.text.dropna())\n",
    "\n",
    "# Build a mapping of numerical ID to word\n",
    "id2word = dict(enumerate(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert our word-matrix into gensim's format\n",
    "corpus = Sparse2Corpus(docs, documents_columns = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit an LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=15)\n",
    "\n",
    "# need to explicitly specify the number of topics we want the model to uncover\n",
    "# num_topics = 10\n",
    "# lda_model = LdaModel(corpus=corpus, \n",
    "#                     id2word=dict(enumerate(vectorizer.get_feature_names())), \n",
    "#                     num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we learn reasonable topics?  \n",
    "Do the words that make up a topic make sense?  \n",
    "Is this topic helpful towards our goal?  \n",
    "We can evaluate fit by viewing the top words in each topic. Some topics will be clearer than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(0, u'0.016*\"like\" + 0.014*\"place\" + 0.011*\"old\" + 0.008*\"new\" + 0.007*\"just\" + 0.007*\"don\" + 0.006*\"ve\" + 0.005*\"know\" + 0.005*\"steak\" + 0.005*\"feel\"')\n",
      "\n",
      "Topic: 1\n",
      "(1, u'0.034*\"fries\" + 0.027*\"cheese\" + 0.026*\"burger\" + 0.022*\"steak\" + 0.022*\"sandwich\" + 0.018*\"good\" + 0.013*\"philly\" + 0.013*\"place\" + 0.009*\"like\" + 0.009*\"burgers\"')\n",
      "\n",
      "Topic: 2\n",
      "(2, u'0.039*\"chicken\" + 0.032*\"pork\" + 0.029*\"fried\" + 0.018*\"ribs\" + 0.015*\"sauce\" + 0.015*\"bbq\" + 0.014*\"beef\" + 0.010*\"belly\" + 0.009*\"sweet\" + 0.009*\"rolls\"')\n",
      "\n",
      "Topic: 3\n",
      "(3, u'0.028*\"breakfast\" + 0.027*\"eggs\" + 0.023*\"sushi\" + 0.022*\"steak\" + 0.018*\"fried\" + 0.013*\"chicken\" + 0.013*\"brunch\" + 0.012*\"good\" + 0.011*\"place\" + 0.010*\"roll\"')\n",
      "\n",
      "Topic: 4\n",
      "(4, u'0.043*\"great\" + 0.039*\"food\" + 0.028*\"place\" + 0.026*\"service\" + 0.020*\"good\" + 0.014*\"steak\" + 0.011*\"staff\" + 0.011*\"amazing\" + 0.011*\"restaurant\" + 0.010*\"time\"')\n",
      "\n",
      "Topic: 5\n",
      "(5, u'0.017*\"food\" + 0.012*\"table\" + 0.011*\"time\" + 0.011*\"minutes\" + 0.010*\"came\" + 0.009*\"service\" + 0.009*\"asked\" + 0.009*\"order\" + 0.009*\"just\" + 0.008*\"server\"')\n",
      "\n",
      "Topic: 6\n",
      "(6, u'0.020*\"ordered\" + 0.018*\"steak\" + 0.013*\"medium\" + 0.012*\"came\" + 0.012*\"rib\" + 0.011*\"rare\" + 0.010*\"dinner\" + 0.010*\"cooked\" + 0.008*\"server\" + 0.008*\"meal\"')\n",
      "\n",
      "Topic: 7\n",
      "(7, u'0.039*\"vegas\" + 0.026*\"meat\" + 0.022*\"las\" + 0.014*\"restaurant\" + 0.010*\"experience\" + 0.009*\"steakhouse\" + 0.008*\"beef\" + 0.008*\"dining\" + 0.007*\"meats\" + 0.007*\"restaurants\"')\n",
      "\n",
      "Topic: 8\n",
      "(8, u'0.013*\"die\" + 0.013*\"und\" + 0.013*\"room\" + 0.009*\"das\" + 0.007*\"hotel\" + 0.007*\"war\" + 0.006*\"es\" + 0.005*\"auch\" + 0.005*\"mit\" + 0.005*\"stay\"')\n",
      "\n",
      "Topic: 9\n",
      "(9, u'0.021*\"beer\" + 0.014*\"buffet\" + 0.013*\"salad\" + 0.013*\"selection\" + 0.010*\"bar\" + 0.010*\"steak\" + 0.009*\"good\" + 0.008*\"pizza\" + 0.007*\"beers\" + 0.007*\"lunch\"')\n",
      "\n",
      "Topic: 10\n",
      "(10, u'0.012*\"menu\" + 0.010*\"dish\" + 0.009*\"sauce\" + 0.008*\"delicious\" + 0.007*\"cheese\" + 0.007*\"dessert\" + 0.007*\"served\" + 0.006*\"cream\" + 0.006*\"chocolate\" + 0.006*\"bread\"')\n",
      "\n",
      "Topic: 11\n",
      "(11, u'0.020*\"great\" + 0.019*\"steak\" + 0.017*\"amazing\" + 0.016*\"service\" + 0.016*\"best\" + 0.013*\"lobster\" + 0.012*\"delicious\" + 0.012*\"filet\" + 0.011*\"food\" + 0.010*\"excellent\"')\n",
      "\n",
      "Topic: 12\n",
      "(12, u'0.029*\"steak\" + 0.025*\"tacos\" + 0.018*\"chicken\" + 0.017*\"rice\" + 0.014*\"food\" + 0.012*\"taco\" + 0.011*\"burrito\" + 0.011*\"good\" + 0.010*\"mexican\" + 0.010*\"salsa\"')\n",
      "\n",
      "Topic: 13\n",
      "(13, u'0.045*\"pho\" + 0.028*\"fajitas\" + 0.026*\"chips\" + 0.026*\"nachos\" + 0.022*\"la\" + 0.018*\"steak\" + 0.012*\"pour\" + 0.011*\"com\" + 0.010*\"que\" + 0.009*\"margarita\"')\n",
      "\n",
      "Topic: 14\n",
      "(14, u'0.037*\"steak\" + 0.032*\"good\" + 0.020*\"food\" + 0.016*\"ordered\" + 0.016*\"like\" + 0.016*\"just\" + 0.013*\"place\" + 0.013*\"really\" + 0.013*\"salad\" + 0.011*\"got\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_topics = 15\n",
    "num_words_per_topic = 10\n",
    "for ti, topic in enumerate(lda_model.show_topics(num_topics = num_topics, num_words = num_words_per_topic)):\n",
    "    print \"Topic: %d\" % (ti)\n",
    "    print topic\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://github.com/RaRe-Technologies/gensim/issues/484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Fewer topics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit an LDA model\n",
    "lda = LdaModel(corpus=corpus, id2word=id2word, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(0, u'0.019*\"great\" + 0.017*\"steak\" + 0.014*\"service\" + 0.012*\"amazing\" + 0.012*\"food\" + 0.011*\"best\" + 0.009*\"delicious\" + 0.009*\"restaurant\" + 0.008*\"good\" + 0.008*\"excellent\"')\n",
      "\n",
      "Topic: 1\n",
      "(1, u'0.014*\"food\" + 0.013*\"ordered\" + 0.012*\"came\" + 0.012*\"steak\" + 0.010*\"table\" + 0.009*\"time\" + 0.009*\"server\" + 0.009*\"service\" + 0.009*\"got\" + 0.008*\"just\"')\n",
      "\n",
      "Topic: 2\n",
      "(2, u'0.033*\"tacos\" + 0.017*\"steak\" + 0.016*\"taco\" + 0.015*\"burrito\" + 0.014*\"mexican\" + 0.013*\"salsa\" + 0.012*\"chips\" + 0.007*\"beans\" + 0.007*\"die\" + 0.007*\"chicken\"')\n",
      "\n",
      "Topic: 3\n",
      "(3, u'0.020*\"steak\" + 0.019*\"good\" + 0.013*\"chicken\" + 0.011*\"cheese\" + 0.010*\"like\" + 0.009*\"ordered\" + 0.009*\"fries\" + 0.008*\"sauce\" + 0.008*\"just\" + 0.007*\"really\"')\n",
      "\n",
      "Topic: 4\n",
      "(4, u'0.024*\"food\" + 0.022*\"place\" + 0.017*\"great\" + 0.014*\"good\" + 0.011*\"steak\" + 0.011*\"service\" + 0.008*\"like\" + 0.008*\"time\" + 0.008*\"bar\" + 0.007*\"just\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_topics = 5\n",
    "num_words_per_topic = 10\n",
    "for ti, topic in enumerate(lda.show_topics(num_topics = num_topics, num_words = num_words_per_topic)):\n",
    "    print \"Topic: %d\" % (ti)\n",
    "    print topic\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.corpora import Dictionary, MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = data['text']\n",
    "texts = [[i for i in doc.lower().split()] for doc in docs]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# gensim.corpora.dictionary.save_as_text(steak_dict)\n",
    "# dictionary = open('steak_dict.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_stopwords():\n",
    "    return set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prep_corpus(texts, additional_stopwords=set(), no_below=5, no_above=0.5):\n",
    "    print('Building dictionary...')\n",
    "    dictionary = Dictionary(texts)\n",
    "    # remove stopwords \n",
    "    stopwords = nltk_stopwords().union(additional_stopwords)\n",
    "    stopword_ids = map(dictionary.token2id.get, stopwords)\n",
    "    # get ids for short words len(word)<=3\n",
    "    shortword_ids = [tokenid for tokenid, word in dictionary.iteritems() if len(word.split('/')[0])<= 3]\n",
    "    dictionary.filter_tokens(stopword_ids)\n",
    "    dictionary.compactify()\n",
    "    # get ids for short words len(word)<=3\n",
    "    shortword_ids = [tokenid for tokenid, word in dictionary.iteritems() if len(word.split('/')[0])<= 3]\n",
    "    dictionary.filter_tokens(shortword_ids)\n",
    "    dictionary.compactify()\n",
    "    # remove words that appear only once\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems()if docfreq == 1]\n",
    "    dictionary.filter_tokens(once_ids)\n",
    "    dictionary.compactify()\n",
    "    # filter extreme values \n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=None)\n",
    "    dictionary.compactify()\n",
    "\n",
    "    print('Building corpus...')\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-eaaccb00f8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/ast.pyc\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'malformed string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/ast.pyc\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'malformed string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: malformed string"
     ]
    }
   ],
   "source": [
    "dictionary ,corpus = prep_corpus([literal_eval(doc) for doc in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 234551 is out of bounds for axis 1 with size 40733",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d6609f4516ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvis_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pyLDAvis/gensim.pyc\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/pyLDAvis/gensim.pyc\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     26\u001b[0m    \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m    \u001b[0mfnames_argsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m    \u001b[0mterm_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_csc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfnames_argsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m    \u001b[0mterm_freqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterm_freqs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m    \u001b[0mdoc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_csc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 234551 is out of bounds for axis 1 with size 40733"
     ]
    }
   ],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_data, 'data/model/vis_data.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
